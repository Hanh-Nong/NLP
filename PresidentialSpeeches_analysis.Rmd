---
title: "Presidential Speeches Analysis - Codes"
author: "Hanh Nong"
date: "12/15/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Set up library, functions and others

```{r library}
options(java.parameters = "- Xmx1024m")
library(Rcpp)
library(text2vec)
library(glmnet)
library(tidytext)
library(widyr)
library(irlba)
library(stringr)
library(Matrix)
library(stm)
library(pals)
library(textdata)
library(sentimentr)
library(socviz)
library(plyr)
library(ggplot2)
library(rvest)
library(openxlsx)
library(rJava)
library(NLP)
library(openNLP)
library(RWeka)
library(reshape2)
library(dplyr)
library(textclean)
library(tm)
library(textstem)
```

```{r POStagging}
#POS tagging function

# creates annotators for words and sentences
word_ann <- Maxent_Word_Token_Annotator()
sent_ann <- Maxent_Sent_Token_Annotator()
pos_ann <- Maxent_POS_Tag_Annotator()

# part of speech annotator
pos_tags <- function(text){
  anno_text <- AnnotatedPlainTextDocument(text, NLP:::annotate(text, list(sent_ann, word_ann, pos_ann)))
  txt <- anno_text$content
  anno <- anno_text$annotation
  
  pos_loc <- sapply(anno$features, `[[`, "POS")
  pos <- pos_loc %in% unique(unlist(pos_loc))
  pos_tag <- unlist(pos_loc[pos])
  wn_tag <- pos_label$wordnet_tag[match(pos_tag,pos_label$Tag)]
  
  word <- txt[anno[pos]]
  pos_term <- paste0(tolower(lemmatize_words(word)), "#",wn_tag)
  pos_score <- as.numeric(sentiwords$score[match(pos_term, sentiwords$lemma)])
  
  sent <- anno[anno$type == "sentence"]
  sent_loc <- c(unlist(mapply(function(x, y) sent$id[sent$start <= x & sent$end >=y],
                              anno[pos]$start,anno[pos]$end)))
  
  data.frame(sentence = sent_loc, POS = pos_tag, wn_tag = wn_tag, 
             pos_word = tolower(word),pos_term = pos_term, score = pos_score)
}
```

```{r colors}
#Set up color for plot
color <- c("Federalist" = "tan1", "Democratic-Republican" = "darkgreen", "Democrat" = "blue", "Whig" = "goldenrod3",  "Republican" = "red")
topic_comp <- c("violence", "shooting", "terror")
color_vst <- c("violence" = "grey70", "shooting" = "grey40", "terror" = "black")
pin <- topic_comp
```

```{r sentiwords}
#Get senti score from sentiwords (https://hlt-nlp.fbk.eu/technologies/sentiwords):
sentiwords <- read.delim("SentiWords_1.1.txt", sep = "\t")
colnames(sentiwords) <- c("lemma", "score")
pos_label <- read.csv("pos_tags.csv", stringsAsFactors = F)
```

# STEP 01: DATA SCRAPING

### Get url in python:

import time
from selenium import webdriver
from bs4 import BeautifulSoup
from urllib.parse import urljoin
driver = webdriver.Chrome(executable_path=r"C:\Program Files\chromedriver.exe")
driver.get("https://millercenter.org/the-presidency/presidential-speeches")
time.sleep(2)
scroll_pause_time = 1
last_height = driver.execute_script("return window.screen.height;")

while True:
    # scroll one screen height each time
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")  
    time.sleep(scroll_pause_time)
    # update scroll height each time after scrolled, as the scroll height can change after we scrolled the page
    new_height = driver.execute_script("return document.body.scrollHeight;")  
    # Break the loop when the height we need to scroll to is larger than the total scroll height
    if new_height == last_height:
        break
    last_height = new_height
    
urls = []
soup = BeautifulSoup(driver.page_source, "html.parser")

for parent in soup.find_all("div", class_="views-row"):
    a_tag = parent.find("a")
    base = "https://millercenter.org"
    link = a_tag.attrs['href']
    url = urljoin(base, link)
    urls.append(url)
    
with open("speeches_links.txt", "w") as f:
    for url in urls:
        f.write(url + "\n")


### Get the html document from url

```{r scrapespeech}
all_links <- read.table("speeches_links.txt", sep = "\n", stringsAsFactors = F)
all_links = unlist(all_links)

scrape_speeches <- function(url){
  html_document <- read_html(url)
  
  transcript_node <- "//*/div[contains(@class, 'transcript-inner')]"
  if(length(html_document %>% html_nodes(xpath = transcript_node)) == 0){
    content_speech <- html_document %>% 
      html_nodes(xpath = "//*/div[contains(@class, 'view-transcript')]") %>% 
      html_text(trim = T)    
  }else{
    content_speech <- html_document %>% 
      html_nodes(xpath = transcript_node) %>% 
      html_text(trim = T)     
  }
  
  president_name <- html_document %>% 
    html_nodes(xpath = "//*/p[contains(@class, 'president-name')]") %>%
    html_text(trim = T)
  
  head_speech <- html_document %>% 
    html_nodes(xpath = "//*/h2[contains(@class, 'presidential-speeches--title')]") %>%
    html_text(trim = T)
  date_speech <- unlist(strsplit(head_speech,":"))[1]
  title_speech <- unlist(strsplit(head_speech,":"))[2]
  
  intro_speech <- html_document %>% 
    html_nodes(xpath = "//*/div[contains(@class, 'about-sidebar--intro')]") %>%
    html_text(trim = T)
  if(length(intro_speech)==0){
    intro_speech <- "not available"
  }
  
  data <- data.frame(president = president_name, 
                     date = date_speech, 
                     title = title_speech,
                     intro = intro_speech, 
                     speech = content_speech)
}

speeches <- data.frame()
for (i in 1:length(all_links)) {
  cat("Downloading", i, "of", length(all_links), "URL:", all_links[i], "\n")
  tmp <- scrape_speeches(all_links[i])
  speeches <- rbind(speeches, tmp)
}
```

# STEP 02: ADD PARTY AND PRESIDENTIAL TERM START DATES

```{r addparty} 
pres_party <- read.csv("pres_party.csv", stringsAsFactors = F) #pres_party.csv obtained from https://en.wikipedia.org/wiki/List_of_presidents_of_the_United_States

colnames(pres_party) <- c("president", "party", "start")
data <- merge(speeches, pres_party, by.x = "president", by.y = "president")
```

# STEP 03: CLEAN DATA

```{r cleandata}
data$spch_id <- 1:nrow(data)

#remove the first "Transcript" term
data$speech <- sub("Transcript", "", data$speech) 

#remove all para breaks
data$speech <- gsub("\n", " ", data$speech)

#format date
data$date <- as.Date(data$date, format = "%B %d,%Y")
data$start <- as.Date(data$start, format = "%B %d,%Y")

#check for duplication
data[duplicated(data$title) | duplicated(data$date) | duplicated(data$date, fromLast = T), c("title", "president", "date", "title", "speech")]

#remove duplicated speeches (10)
dup <- c(61, 123, 127, 132, 135, 141, 418, 485, 963, 987)
data <- data[-dup,]

#remove speeches before presidential term start (61)
data <- data[data$date >= data$start,]

#prepare data: remove all 'applause' and 'booo'
data$speech <- gsub("(applause)", "", data$speech, ignore.case = T)
data$speech <- gsub("(booo)", "", data$speech, ignore.case = T)
```

# STEP 04: ADD TOPICS BASED ON TITLE

```{r titop}
titop <- read.csv("title_topic.csv", stringsAsFactors = F) #title_topic.csv is manually prepared
data$titop <- titop$topic[match(data$title, titop$title)]
```

# STEP 05: EDA BEFORE PROCESSING

```{r eda_nbr_spch_pres}
eda <- data %>% group_by(president, party, start) %>% 
  summarise(n = n())
ggplot(data = eda, aes(x = reorder(president,eda$start), y = n, fill = party)) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.4, hjust = 1), legend.position="right") +
  geom_bar(stat = "identity") +
  scale_fill_manual(values = color, name = "Party") +
  labs(title = "Number of speeches by each president",
       x = "Presidents", y = "Number of speeches")
```

```{r eda_nbr_presspch_party}
eda <- data %>% group_by(party) %>% mutate(n_pres = n_distinct(president)) %>% summarise(n_pres = max(n_pres))
ggplot(data = eda, aes(x = reorder(party, n_pres), y = n_pres, fill = party)) +
  geom_bar(stat = "identity", width = 0.5) + coord_flip() +
  theme(axis.text.y = element_blank(),legend.position = "bottom") +
  scale_fill_manual(values = color, name = "Party") +
  labs(title = "Number of presidents by party",
       x = "Parties", y = "Number of presidents")

eda <- data %>% group_by(party) %>%  summarise(n = n())
ggplot(data = eda, aes(x = reorder(party,n), y = n, fill = party)) +
  geom_bar(stat = "identity", width = 0.5) + coord_flip() +
  theme(axis.text.y = element_blank(), legend.position = "bottom") +
  scale_fill_manual(values = color, name = "Party") +
  labs(title = "Number of speeches by party",
       x = "Parties", y = "Number of speeches")
```

```{r eda_nbr_spch_type}
eda <- data[data$titop != "miscelanous",] %>% group_by(titop) %>%  summarise(n = n())
eda <- eda[order(eda$n, decreasing = T),]
ggplot(data = eda[1:30,], aes(x = reorder(titop,n), y = n)) +
  geom_bar(stat = "identity") + coord_flip() +
  labs(title = "Top categories by number of speeches",
       x = "Category", y = "Number of speeches")
```

# STEP 06: GET SENTIMENT SCORE FROM SENTIWORDS

```{r pos_whole}
#Get POS and sentiwords score for each term for the whole corpus
pos_tags_corpus <- data.frame()
for(i in 1:nrow(data)){
  pos_pres <- pos_tags(data$speech[i])
  pos_pres$president <- data$president[i]
  pos_pres$party <- data$party[i]
  pos_pres$date <- data$date[i]
  pos_pres$title <- data$title[i]
  pos_tags_corpus <- rbind(pos_tags_corpus, pos_pres)
}
write.csv(pos_tags_corpus, "pos_tags_corpus.csv", row.names = F)
```

### Get lexicographer file name from WordNet in Python:

This step is necessary in order to match words in speech data to words in `SentiWords` using lexicographer file name.

Below are codes in Python:

import nltk
from nltk.corpus import wordnet as wn
from nltk.corpus import sentiwordnet as swn
import pandas as pd
import numpy as np
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"

pos_tags = pd.read_csv("pos_tags.csv")
pos_lex_corpus = pd.read_csv("pos_tags_corpus.csv", encoding= 'unicode_escape')
pos_lex_corpus['wn_tag'] = pos_lex_corpus['POS'].map(pos_tags.set_index('Tag')['wordnet_tag'])
pos_lex_corpus['wn_tag'] = pos_lex_corpus['wn_tag'].fillna('o')
pos_lex_corpus['pos_word'] = pos_lex_corpus['pos_word'].fillna('n_a')

lexname = []
for i in range(len(pos_lex_corpus)):
    synset = pd.DataFrame(wn.synsets(pos_lex_corpus.loc[i,'pos_word']))
    if synset.empty == False:
        synset['type'] = synset[0].astype(str).str.split('.').str[1]    
        if sum(synset['type'] == pos_lex_corpus.loc[i,'wn_tag']) != 0:
            lexname.append(synset.loc[np.where(synset['type'] == pos_lex_corpus.loc[i,'wn_tag'])[0][0],0].lexname())
        else:
            lexname.append("not_"+str(pos_lex_corpus.loc[i,'wn_tag']))
    else:
        lexname.append('notfound')
pos_lex_corpus['lexname'] = lexname
pos_lex_corpus.to_csv('pos_lex_corpus.csv',index=False)

### Absolute value for sentiment score

```{r pos_abs}
#pos_lex_corpus is the pos tagged corpus after getting lexname in python

#Get absolute value for each term
pos_lex_corpus$abs_score <- abs(pos_lex_corpus$score)

#Get sum of absolute value for each speech
pos_lex_abs <- data.frame()
anchor <- nrow(pos_lex_corpus) - 1170 #1170 is the word count of the last speech in data
i <- 1
while(i <= anchor+1){
  p1 <- pos_lex_corpus$president[i]
  p2 <- pos_lex_corpus$party[i]
  p3 <- pos_lex_corpus$date[i]
  p4 <- pos_lex_corpus$title[i]
  temp <- pos_lex_corpus[pos_lex_corpus$president == p1 &
                         pos_lex_corpus$party == p2 &
                         pos_lex_corpus$date == p3 &
                         pos_lex_corpus$title == p4, ]
  df <- data.frame(president = p1, party = p2, date = p3, title = p4, abs_sw = sum(temp$abs_score, na.rm = T))
  pos_lex_abs <- rbind(pos_lex_abs, df)
  i <- i + nrow(temp)
}

#Attach absolute sentiwords scores to original data
data$sw_abs <- pos_lex_abs$abs_sw
```

# STEP 07: NORMALIZE SENTIMENT SCORES FOR SENTENCES

```{r norm}
#Get number of sentences for each speech
data$sent_count <- lengths(get_sentences(data$speech))

#normalize:
data$sw_norm <- data$sw_abs/data$sent_count
```

# STEP 08: STRENGTH OF OPINION

```{r partydiff}
#Get the mean of absolute sentiment scores for each party
eda <- data %>% group_by(titop, party) %>% summarise(sw = mean(sw_norm))

#transform to wide to get difference between two parties
eda_cast <- dcast(eda, titop~party)
eda_cast$diff <- eda_cast$Democrat - eda_cast$Republican

#extract data of Dem and Rep
eda_DR <- eda[eda$party == "Democrat" | eda$party == "Republican",]

#merge party difference to data of Dem and Rep to have data in long form
eda_cast_DR <- merge(eda_DR, eda_cast[,c("titop", "diff")])

#prepare data for a symmetric plot of all topics' difference:
##divide score difference into two 
eda_cast_DR$diff_score <- eda_cast_DR$diff/2
##multiply all Republican with -1. 
##If Republican is weaker (diff >0), it will be on the left, else it will be on the right
eda_cast_DR[eda_cast_DR$party == "Republican", "diff_score"] <- eda_cast_DR[eda_cast_DR$party == "Republican", 
                                                                            "diff_score"]*(-1)
eda_cast_DR <- na.omit(eda_cast_DR)
```

```{r plot_partydistance}
## ---- plot4norm_partydistance --------
#set boundaries for colored rectangles
bound <- length(unique(eda_cast_DR$titop))
x_min <- c(seq(from = 0.5, to = bound-0.5, by = 1))
x_max <- c(seq(from = 1.5, to = bound+0.5, by = 1))

#remove all rows in wide data that has NA diff. This is to prepare colored rectangle for the plot
eda_cast <- eda_cast[complete.cases(eda_cast[ , "diff"]),]

#set up colored rectangle for stronger party
rect_color <- ifelse(eda_cast[order(abs(eda_cast$diff)),"diff"] < 0, "red", "blue")

#plot
ggplot(data = eda_cast_DR, aes(x = reorder(titop,abs(diff)), y = diff_score, color = party)) +
  geom_point(size = 3) + 
  scale_color_manual(values = color) +
  coord_flip()+ 
  ggplot2::annotate(geom = "rect", xmin = x_min, xmax = x_max,
                    ymin = -Inf, ymax = Inf,fill = rect_color, alpha = 0.2)+
  labs(title = "Difference in strength of opinion",
       x = "Topic", y = "Distance between two parties", color = "Party") +
  theme(axis.text.x = element_blank())
```

# STEP 09: EMOTION FREQUENCY

```{r emo_corpus}
#extract emotion count
emo <- emotion(get_sentences(data$speech))
emo$president <- data$president[match(emo$element_id, 1:nrow(data))]
emo$party <- data$party[match(emo$element_id, 1:nrow(data))]
emo$title <- data$title[match(emo$element_id, 1:nrow(data))]
emo$date <- data$date[match(emo$element_id, 1:nrow(data))]
write.csv(emo, "emo_corpus.csv", row.names = F)
```

```{r data_emofreq_plot}
#Get the count of meaningful words for each speech
pos_tags_ct <- pos_lex_corpus[pos_lex_corpus$wn_tag %in% c("n", "v", "a", "r"),] %>% group_by(president, party, date, title) %>% count()

#Sum the count of emotion terms for each emotion for each speech
#emotion() from sentimentr returns count at sentence level, hence it must be summarised by speech
eda_gen <- emo %>% group_by(president, party, date, title, element_id) %>% summarise(emo_ct = sum(emotion_count))

#Get the count of meaningful words of the associated speech for each emotion
eda_gen$word_ct <- pos_tags_ct$n

#Add start date and topics to visualize results for all topics in chronological order
eda_gen$start <- as.Date(data$start[match(eda_gen$president, data$president)])
```

# STEP 10: SW FOR EMOTION TERMS

### First, extract emotion terms

```{r emo_terms}
emo_terms <- extract_emotion_terms(get_sentences(data$speech))
```

### Second, get SentiWords score for emotion terms

```{r emo_terms_sw}
#initiate dataframe to store each speech's emotion term scores
pos_lex_emo <- data.frame()
anchor <- nrow(pos_lex_corpus) - 1170 #1170 is the word count of last speech in data
i <- 1
spch_id <- 1
emo_type <- colnames(emo_terms[,3:10])

while(i <= anchor+1){
  p1 <- pos_lex_corpus$president[i]
  p2 <- pos_lex_corpus$party[i]
  p3 <- pos_lex_corpus$date[i]
  p4 <- pos_lex_corpus$title[i]
  
  #initiate dataframe for the current speech
  temp <- pos_lex_corpus[pos_lex_corpus$president == p1 &
                           pos_lex_corpus$party == p2 &
                           pos_lex_corpus$date == p3 &
                           pos_lex_corpus$title == p4, ]
  
  #extract emo_terms for the current speech
  emo_temp <- emo_terms[emo_terms$element_id == spch_id,]
  
  #create dataframe to store total score at sentence level for each of eight emotions
  t_df <- data.frame(matrix(ncol = 8, nrow = 1))
  colnames(t_df) <- emo_type
  
  #loop through each emotion type
  for(t in emo_type){
    #flatten character in emo type of current sentence
    emo_char <- unlist(emo_temp[, get(t)], use.names = F)
    #label terms in current sentence that are in emo_terms
    temp$emo <- ifelse(lemmatize_words(temp$pos_word) %in% emo_char, t,"") 
    #get score of found emo terms
    t_df[, t] <- sum(temp$score[temp$emo == t], na.rm = T)
  }
  df <- data.frame(president = p1, party = p2, date = p3, title = p4)
  #append sum of score of emotion terms for each speech
  df[,emo_type] <- colSums(t_df, na.rm = T)
  pos_lex_emo <- rbind(pos_lex_emo, df)
  i <- i + nrow(temp)
}

pos_lex_emo$element_id <- 1:nrow(pos_lex_emo)
```

### Third, prepare data for plot

```{r data_emoscore_plot}
pos_lex_emo <- pos_lex_emo[,!(colnames(pos_lex_emo) %in% c("word_count", "sent_count"))]

#transform emo data to long form
emo_sw <- melt(pos_lex_emo, id.vars = c("president", "party", "date", "title", "element_id"))

#get emotion term count to prepare for getting mean score
eda_gen_sw <- emo %>% group_by(president, party, date, title, element_id, emotion_type) %>% 
  summarise(emo_ct = sum(emotion_count, na.rm = T))

emo_sw$emoct <- eda_gen_sw$emo_ct[match(paste(emo_sw$element_id, emo_sw$variable), 
                                        paste(eda_gen_sw$element_id, eda_gen_sw$emotion_type))]

#get mean of sentiwords score for each emotion of speech
emo_sw$sw_mean <- emo_sw$value/emo_sw$emoct
emo_sw$sw_mean[is.na(emo_sw$sw_mean)] <- 0
emo_sw$sw_mean[is.infinite(emo_sw$sw_mean)] <- 0

#Add start date, date and topic for visualize all topics in chronological order
emo_sw$start <- as.Date(data$start[match(emo_sw$president, data$president)])
emo_sw$date <- as.Date(emo_sw$date)
emo_sw$ntitle <- emo$title[match(emo_sw$element_id, emo$element_id)] #this is because can't match topic directly from emo_sw to data due to unicode problem
emo_sw$titop <- data$titop[match(emo_sw$ntitle, data$title)]
```

# STEP 11: EMOTION PLOT

### Frequency and scores for all emotions as a whole, by each president

```{r plot_emo_prop_sw_whole}

#Frequency
eda <- eda_gen
eda$prop <- eda$emo_ct/eda$word_ct

ggplot(data = eda %>% group_by(party, president, start) %>% summarise(prop_mean = mean(prop)), 
       aes(x = reorder(president, start), y = prop_mean, fill = party)) +
  geom_bar(stat = 'identity') + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.4, hjust = 1), legend.position = "right") +
  #theme(axis.text.x = element_blank(), axis.title.x=element_blank(), legend.position = "right") +
  labs(title = "Emotion term frequency by president",
       #subtitle = "All topics",
       x = "President", 
       y = "Proportion of emotion terms", 
       fill = "Party")+
  scale_fill_manual(values = color)

#Scores
emo_sw$sw_meanabs <- abs(emo_sw$sw_mean)
eda_emowhole <- emo_sw %>% group_by(president, party, start, element_id) %>% 
  summarise(score = mean(sw_meanabs))

ggplot(data = eda_emowhole %>% group_by(president, party, start) %>% 
         summarise(score = mean(score)), 
       aes(x = reorder(president, start), y = score, fill = party)) +
  geom_bar(stat = 'identity') + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.4, hjust = 1), legend.position = "right") +
  labs(title = "Emotion term scores by president",
       #subtitle = "All topics",
       x = "President", y = "Score", 
       fill = "Party")+
  scale_fill_manual(values = color)
```

### Frequency and scores by emotion, for all topics

```{r plot_emo_prop_sw_type}
#prop
eda <- emo[emo$party %in% c("Democrat", "Republican") & !grepl("negated", emo$emotion_type),] %>% 
  group_by(president, party, date, title, element_id, emotion_type) %>% summarise(emo_ct = sum(emotion_count))
eda$start <- as.Date(data$start[match(eda$president, data$president)])
eda$prop <- eda$emo_ct/eda_gen$word_ct[match(eda$element_id, eda_gen$element_id)]
eda$titop <- data$titop[match(eda$title, data$title)]

ggplot(data = eda, 
       aes(x = start, y = prop, 
           color = titop)) +
  geom_point(size = 1)+ 
  geom_smooth(method='lm',se = T, color = "grey50") +
  theme(axis.text.x = element_blank(), legend.position = "none", axis.title.x = element_blank()) +
  labs(title = "Emotion term proportion from 1789 to 2020 - All topics", 
       #subtitle = "Summarised by topics",
       #x = "From 1789 to 2020", 
       y = "Proportion of emotion terms", 
       fill = "Party")+
  coord_cartesian(ylim = c(0, 0.2)) +
  facet_wrap(~emotion_type, ncol = 8)

#score
ggplot(data = emo_sw, 
       aes(x = start, y = sw_mean, 
           color = titop)) +
  geom_point(size = 1)+
  geom_smooth(method='lm',se = T, color = "grey50") +
  theme(axis.text.x = element_blank(), legend.position = "none", axis.title.x = element_blank()) +
  labs(title = "Emotion term scores from 1789 to 2020 - All topics", 
       #subtitle = "All topics",
       #x = "From 1789 to 2020", 
       y = "Score", 
       fill = "Party")+
  coord_cartesian(ylim = c(-0.3, 0.4)) +
  facet_wrap(~variable, ncol = 8)
```

### Frequency and scores by emotion, for topics of interest as a whole, compared with other topics

```{r plot_emo_prop_sw_individual}

## ---- prop --------
eda <- emo[emo$party %in% c("Democrat", "Republican") & !grepl("negated", emo$emotion_type),] %>% 
  group_by(president, party, date, title, element_id, emotion_type) %>% summarise(emo_ct = sum(emotion_count))
eda$start <- as.Date(data$start[match(eda$president, data$president)])
eda$prop <- eda$emo_ct/eda_gen$word_ct[match(eda$element_id, eda_gen$element_id)]
eda$titop <- data$titop[match(eda$title, data$title)]
eda$color <- ""
for(t in 1:nrow(eda)){
  if(eda$titop[t] %in% pin){
    #eda$color[t] <- color[[eda$party[t]]]
    eda$color[t] <- "deeppink"
  }else{eda$color[t] <- "grey80"}
}

subtitle_pin <- paste0(pin, collapse = ", ")

ggplot(data = eda, 
       aes(x = reorder(president, start), y = prop, 
           color = titop)) +
  geom_point(size = 1, color = eda$color)+ 
  geom_smooth(method='lm', formula = prop ~ start,se = FALSE, color = "green") +
  theme(axis.text.x = element_blank(), legend.position = "none", axis.title.x = element_blank()) +
  labs(title = paste("Emotion term proportion from 1789 to 2020 -",subtitle_pin), 
       y = "Proportion of emotion terms", 
       fill = "Party")+
  coord_cartesian(ylim = c(0, 0.2)) +
  facet_wrap(~emotion_type, ncol = 8)

## ---- score --------
for(t in 1:nrow(emo_sw)){
  if(emo_sw$titop[t] %in% pin){
    #eda$color[t] <- color[[eda$party[t]]]
    emo_sw$colour[t] <- "deeppink"
  }else{emo_sw$colour[t] <- "grey80"}
}
subtitle_pin <- paste0(pin, collapse = " ")
ggplot(data = emo_sw, 
       aes(x = reorder(president, start), y = sw_mean, 
           color = titop)) +
  geom_point(size = 1, color = emo_sw$colour)+
  theme(axis.text.x = element_blank(), legend.position = "none", axis.title.x = element_blank()) +
  labs(title = paste("Emotion term scores from 1789 to 2020 -",subtitle_pin), 
       y = "Score", 
       fill = "Party")+
  coord_cartesian(ylim = c(-0.3, 0.4)) +
  facet_wrap(~variable, ncol = 8)
```

### Frequency and scores by emotion, for topics of interest as a whole, only show relevant presidents

```{r plot_emo_vst_type_pres}
#prop
eda <- emo[!grepl("negated", emo$emotion_type),] %>% 
  group_by(president, party, title, element_id, emotion_type) %>% summarise(emo_ct = sum(emotion_count))
eda$start <- as.Date(data$start[match(eda$president, data$president)])
eda$prop <- eda$emo_ct/eda_gen$word_ct[match(eda$element_id,eda_gen$element_id)]

eda$topic <- data$titop[match(eda$title,data$title)]
eda <- eda[eda$topic %in% topic_comp, ]

ggplot(data = eda, aes(x = reorder(president, start), y = prop, fill = party)) +
  geom_bar(stat = "identity")+ 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.4, hjust = 1), legend.position = "right") +
  labs(title = "Emotion terms by emotion type", 
       x = "President", 
       y = "Proportion of emotion terms", 
       fill = "Party")+
  scale_fill_manual(values = color)+
  facet_wrap(~emotion_type, ncol = 8)

#score
eda <- emo_sw[emo_sw$titop %in% topic_comp, ] %>% 
  group_by(president, party, start, variable) %>% summarise(swmean = mean(sw_mean))

ggplot(data = eda, aes(x = reorder(president, start), y = swmean, fill = party)) +
  geom_bar(stat = "identity")+ 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.4, hjust = 1), legend.position = "right") +
  labs(title = "Emotion term score by emotion type", 
       x = "President", y = "Mean score", 
       fill = "Party")+
  scale_fill_manual(values = color)+
  facet_wrap(~variable, ncol = 8)
```

### Frequency and scores by emotion, by topics of interest

```{r plot_emo_vst_pres}
#prop
eda <- emo[!grepl("negated", emo$emotion_type),] %>% 
  group_by(president, title, element_id, emotion_type) %>% summarise(emo_ct = sum(emotion_count))
eda$start <- as.Date(data$start[match(eda$president, data$president)])
eda$prop <- eda$emo_ct/eda_gen$word_ct[match(eda$element_id,eda_gen$element_id)]

eda$topic <- data$titop[match(eda$title,data$title)]
eda <- eda[eda$topic %in% topic_comp, ]

ggplot(data = eda, aes(x = reorder(president, start), y = prop, color = topic)) +
  geom_point(aes(shape = topic), size = 3) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.4, hjust = 1), legend.position = "bottom") +
  labs(title = "Emotion term frequency by presidents", 
       subtitle = paste("Topic: ", subtitle_pin),
       x = "President", y = "Proportion", 
       fill = "Topic")+ 
  facet_wrap(~emotion_type, ncol = 8)

#score
eda <- emo_sw[emo_sw$titop %in% topic_comp, ] %>% 
  group_by(president, start, title, titop, variable) %>% summarise(swmean = mean(sw_mean))

eda$topic <- data$titop[match(eda$title,data$title)]

ggplot(data = eda, aes(x = reorder(president, start), y = swmean, color = topic)) +
  geom_point(aes(shape = topic), size = 3) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.4, hjust = 1), legend.position = "bottom") +
  labs(title = "Emotion sentiment scores by president", 
       subtitle = paste("Topic: ", subtitle_pin),
       x = "President", y = "Mean sentiment", 
       fill = "Topic") +
  #scale_color_manual(values = color_vst)+
  facet_wrap(~variable, ncol = 8)
```




















